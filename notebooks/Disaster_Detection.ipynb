{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook will walk you through the process of building the simple neural model for detection of disasterous Tweets. Keras documentation might be helful with the excercises: https://keras.io/.\n",
    "\n",
    "First step is importing required python packages, and fixing the random seeds for reproducible experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Embedding, Dense, SimpleRNN\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed settings for repeatable experiments\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset loading\n",
    "\n",
    "In the next step, we will load the dataset of Disasterous and Casual Tweets. These tweets can be loaded from the provided .tsv file. Disaster Tweets are labeled with `1`, and Casual - with `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TextDataset = namedtuple('TextDataset', ('texts', 'labels'))\n",
    "\n",
    "def load_disaster_dataset():\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    data_path=os.path.join('data', 'disasters_socialmedia.tsv')\n",
    "    with open(data_path, encoding='utf8') as data_file:\n",
    "        for line in tqdm(list(data_file)[1:]): #wrap in progress bar, skip the headers\n",
    "            tweet, _, label = line.strip().split('\\t')\n",
    "            label = int(label)\n",
    "            if label == 2: # Skip Can't decide (2) category\n",
    "                continue\n",
    "            tweets.append(tweet)\n",
    "            labels.append(label)\n",
    "    return TextDataset(texts=tweets, labels=labels)\n",
    "\n",
    "disasters_dataset = load_disaster_dataset()\n",
    "\n",
    "disasterous = [disasters_dataset.texts[i] for i, label in enumerate(disasters_dataset.labels) \n",
    "                      if label == 1]\n",
    "\n",
    "casual = [disasters_dataset.texts[i] for i, label in enumerate(disasters_dataset.labels) \n",
    "                      if label == 0]\n",
    " \n",
    "print('Fraction of casual tweets: {:.3f}'.format(len(casual)/(len(disasterous)+len(casual))))\n",
    "\n",
    "print('\\n==================\\nDISASTEROUS TWEETS\\n==================')\n",
    "for tweet in random.sample(disasterous, 5):\n",
    "    print('------\\n',tweet)\n",
    "\n",
    "\n",
    "print('\\n==================\\nCASUAL TWEETS\\n==================')\n",
    "\n",
    "for tweet in random.sample(casual, 5):\n",
    "    print('------\\n',tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data encoding\n",
    "\n",
    "After loading the textual data, we have to transform it to numerical form, understandable by the neural model. Keras require the data to be indexed (every unique word -> unique index) and padded to the same length. We will use `keras.preprocessing.text.Tokenizer` and `keras.preprocessing.sequence` tools for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data preparation (probably should be more explicit, here in Notebook)\n",
    "\n",
    "EncodedDataset = namedtuple('EncodedDataset', ('instances', 'labels'))\n",
    "\n",
    "def encode_dataset(text_dataset, tokenizer):\n",
    "    texts_enco = tokenizer.texts_to_sequences(text_dataset.texts)\n",
    "    texts_padded = sequence.pad_sequences(texts_enco, padding='pre')\n",
    "    encoded_dataset = EncodedDataset(instances=texts_padded, labels=text_dataset.labels)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    sequence_len = len(texts_padded[0])\n",
    "    return encoded_dataset, vocab_size, sequence_len\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True,\n",
    "                      split=' ',\n",
    "                      char_level=False)\n",
    "tokenizer.fit_on_texts(disasters_dataset.texts)\n",
    "encoded_dataset, vocab_size, sequence_len = encode_dataset(disasters_dataset, tokenizer)\n",
    "print('Vocab size: {}, Sequence len: {}'.format(vocab_size, sequence_len))\n",
    "print('\\nEncoded Tweets:')\n",
    "for instance in encoded_dataset.instances[:10]:\n",
    "    print(instance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing training/validation splits\n",
    "\n",
    "Now, we will split our dataset to two parts: \n",
    "- one used for training (75%) and \n",
    "- one used for validation (assesment of the accuracy of the model, 25%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(dataset, valid_fraction=0.25):\n",
    "    data_size = len(dataset.instances)\n",
    "    split_id = int(data_size * valid_fraction)\n",
    "    merged_data = list(zip(dataset.instances, dataset.labels))\n",
    "    random.shuffle(merged_data)\n",
    "    shuffled_instances, shuffled_labels = zip(*merged_data)\n",
    "    valid_data = EncodedDataset(instances=shuffled_instances[0:split_id],\n",
    "                                labels=shuffled_labels[0:split_id])\n",
    "    train_data = EncodedDataset(instances=shuffled_instances[split_id:],\n",
    "                                labels=shuffled_labels[split_id:])\n",
    "    return train_data, valid_data\n",
    "\n",
    "train_data, valid_data = get_splits(encoded_dataset)\n",
    "\n",
    "print('Training data size: {}, Validation data size: {}'.format(len(train_data.labels), len(valid_data.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building and training the neural model\n",
    "\n",
    "We will train a simple RNN model and train in with SGD algorithm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Building model...', end=' ')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_length=sequence_len, input_dim=vocab_size, output_dim=50, mask_zero=True)) \n",
    "# Set mask_zero=False when using CNN\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "print('done')\n",
    "\n",
    "print('Training model..')\n",
    "x_train = np.array(train_data.instances, np.float32)\n",
    "y_train = np.array(train_data.labels, np.int32)\n",
    "x_valid = np.array(valid_data.instances, np.float32)\n",
    "y_valid = np.array(valid_data.labels, np.int32)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=32, epochs=5, verbose=2)\n",
    "print('Training comleted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets evaluate our trained model once again on validatation split, to confirm the final accuracy of the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final validation..\")\n",
    "_, acc = model.evaluate(x_valid, y_valid, verbose=2)\n",
    "print(\"Accuracy on validation set: {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (warmup): Display performance on Training set \n",
    "\n",
    "Similarly to validation split evaluation, please evalate your model on training data and examine the performance. The difference between training and validation accuracy will give you some insight about generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2: Improve network performance\n",
    "\n",
    "Default network architecture should achieve ~60% accuracy on the Disasters dataset. It's better than random, but still not satisfactory. Please use the knowledge acquired during presentation (about different network architectures, optimization algorithms, etc) to improve the model. Try to reach 80% accucary, or even more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3: Generate network predictions for example text data\n",
    "\n",
    "Please write simple code that will generate predictions for the list of custom sentences, for example:\n",
    "```\n",
    "texts = ['Fiery 14-vehicle crash on Highway 400 kills 3',\n",
    "         'I have a crush on Monica']\n",
    "```\n",
    "the output should look like this:\n",
    "\n",
    "```\n",
    "1.000 -> Fiery 14-vehicle crash on Highway 400 kills 3\n",
    "0.000 -> I have a crush on Monica\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Hits:\n",
    "- The example texts should be processed in the similar way as we processed training data\n",
    "    - Use to `keras.preprocessing.sequence` with `sequence_len` parameter filled to  pad encoded data to desired length:\n",
    "```\n",
    "keras.preprocessing.sequence.pad_sequences(sequences=..., sequence_len=desired_len)\n",
    "``` \n",
    "\n",
    "- Use `model.predict()` method to generate predictions for the data \n",
    "- The `EarlyStopping` callback can be used to stop training when the validation score stops increasing:\n",
    "\n",
    "```\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', mode='max')\n",
    "model.fit(x=..., y=..., epochs=100, callbacks=[early_stopping])\n",
    "```\n",
    "  Another option is to reduce the decrease the number of epochs to have the best model trained in the last epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 4: Display misclassified examples for error analysis\n",
    "\n",
    "Manual inspection of misclassified examples (error analysis) is a good way to get insight about the model behaviour and source of ideas for possible improvements. Using the code from previous excercise, try to display misclassified examples from validation data and see whether you see any patterns in the errors that the network is doing. Assume that `Disasterous` label is chosen when the prediction is > 0.5, otherwise the tweet is treated as `Casual`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
