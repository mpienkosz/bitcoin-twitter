{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Embedding, Dense, SimpleRNN\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras import backend\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import data_utils\n",
    "\n",
    "# Set random seed settings for repeatable experiments\n",
    "#os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10876/10876 [00:00<00:00, 483658.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 28980, Sequence len: 33\n"
     ]
    }
   ],
   "source": [
    "# Data preparation (probably should be more explicit, here in Notebook)\n",
    "disasters_dataset = data_utils.load_disaster_dataset()\n",
    "encoded_dataset, vocab_size, sequence_len = data_utils.encode_dataset(disasters_dataset)\n",
    "train_data, valid_data = data_utils.get_splits(encoded_dataset)\n",
    "print('Vocab size: {}, Sequence len: {}'.format(vocab_size, sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model... done\n",
      "Training model..\n",
      "Train on 8145 samples, validate on 2715 samples\n",
      "Epoch 1/1\n",
      "8145/8145 [==============================] - 6s 740us/step - loss: 0.6768 - acc: 0.5862 - val_loss: 0.6736 - val_acc: 0.5912\n",
      "done\n",
      "done\n",
      "Training model..\n",
      "Train on 8145 samples, validate on 2715 samples\n",
      "Epoch 1/1\n",
      "8145/8145 [==============================] - 6s 788us/step - loss: 0.6746 - acc: 0.5944 - val_loss: 0.6754 - val_acc: 0.5867\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('Building model...', end=' ')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_length=sequence_len, input_dim=vocab_size, output_dim=50))\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "print('done')\n",
    "\n",
    "print('Training model..')\n",
    "x_train = np.array(train_data.instances, np.float32)\n",
    "y_train = np.array(train_data.labels, np.int32)\n",
    "x_valid = np.array(valid_data.instances, np.float32)\n",
    "y_valid = np.array(valid_data.labels, np.int32)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=32, epochs=1)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final validation..\")\n",
    "_, acc = model.evaluate(x_valid, y_valid)\n",
    "print(\"Accuracy on validation set: {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
